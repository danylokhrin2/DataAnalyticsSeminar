{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conufsion Matrix and Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "results = []\n",
    "confusion_values = []\n",
    "\n",
    "model_columns = [\"gpt-4o-mini\", \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\", \"llama3.2-3b\", \"llama3.1-8b\", \"llama3-8b\"]\n",
    "\n",
    "for model in model_columns:\n",
    "    # Cast SEXISM to int\n",
    "    y_true = df['SEXISM'].astype(float).astype(int)\n",
    "    df[model] = pd.to_numeric(df[model], errors='coerce').fillna(0).astype(int)\n",
    "    y_pred = df[model]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"AUC\": auc\n",
    "    })\n",
    "    \n",
    "    confusion_values.append({\n",
    "        \"Model\": model,\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "confusion_df = pd.DataFrame(confusion_values)\n",
    "\n",
    "print(\"Confusion Matrix Values:\")\n",
    "print(confusion_df)\n",
    "print(\"\\nMetrics:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "results = []\n",
    "\n",
    "model_columns = [\"gpt-4o-mini\", \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\", \"llama3.2-3b\", \"llama3.1-8b\", \"llama3-8b\"]\n",
    "\n",
    "for model in model_columns:\n",
    "    # Cast SEXISM to int\n",
    "    y_true = df['SEXISM'].astype(float).astype(int)\n",
    "    df[model] = pd.to_numeric(df[model], errors='coerce').fillna(0).astype(int)\n",
    "    y_pred = df[model]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"AUC\": auc\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_melted = results_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "mean_scores = results_melted.groupby(\"Metric\")[\"Score\"].mean()\n",
    "plot_order = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC\"]\n",
    "mean_scores = mean_scores.reindex(plot_order)\n",
    "print(mean_scores)\n",
    "\n",
    "palette = {\n",
    "    \"gpt-4o-mini\": \"#1FDCB0\",\n",
    "    \"gpt-4-0125-preview\": \"#16A180\",\n",
    "    \"gpt-3.5-turbo-0125\": \"#0C5443\",\n",
    "    \"llama3.2-3b\": \"#0081FB\",\n",
    "    \"llama3.1-8b\": \"#005AAD\",\n",
    "    \"llama3-8b\": \"#003669\"\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(data=results_melted, x=\"Metric\", y=\"Score\", hue=\"Model\", palette=palette)\n",
    "\n",
    "for i, (metric, mean) in enumerate(mean_scores.items()):\n",
    "    plt.plot([i - 0.4, i + 0.4], [mean, mean], color='black', linestyle='-', linewidth=1.5, label=f\"Average\" if i == 0 else None)\n",
    "\n",
    "\n",
    "plt.title(\"Performance Metrics Across Models\", fontsize=18)\n",
    "plt.xlabel(\"Metric\", fontsize=18)\n",
    "plt.ylabel(\"Score\", fontsize=18)\n",
    "plt.xticks(rotation=45, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(title=\"Model\", fontsize=16, title_fontsize=18, bbox_to_anchor=(0.6, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/performance_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "confusion_values = []\n",
    "\n",
    "model_columns = [\"gpt-4o-mini\", \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\", \"llama3.2-3b\", \"llama3.1-8b\", \"llama3-8b\"]\n",
    "\n",
    "for model in model_columns:\n",
    "    # Cast SEXISM to int\n",
    "    y_true = df['SEXISM'].astype(float).astype(int)\n",
    "    df[model] = pd.to_numeric(df[model], errors='coerce').fillna(0).astype(int)\n",
    "    y_pred = df[model]\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    confusion_values.append({\n",
    "        \"Model\": model,\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "total_sexist = (df['SEXISM'] == 1).sum()\n",
    "total_non_sexist = (df['SEXISM'] == 0).sum()\n",
    "confusion_df = pd.DataFrame(confusion_values)\n",
    "confusion_melted = confusion_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "palette = {\n",
    "    \"gpt-4o-mini\": \"#1FDCB0\",\n",
    "    \"gpt-4-0125-preview\": \"#16A180\",\n",
    "    \"gpt-3.5-turbo-0125\": \"#0C5443\",\n",
    "    \"llama3.2-3b\": \"#0081FB\",\n",
    "    \"llama3.1-8b\": \"#005AAD\",\n",
    "    \"llama3-8b\": \"#003669\"\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(data=confusion_melted, x=\"Metric\", y=\"Value\", hue=\"Model\", palette=palette)\n",
    "\n",
    "metrics = confusion_melted[\"Metric\"].unique()\n",
    "metric_positions = {metric: i for i, metric in enumerate(metrics)}\n",
    "\n",
    "for metric, total_count in [(\"TP\", total_sexist), (\"TN\", total_non_sexist)]:\n",
    "    if metric in metric_positions:\n",
    "        i = metric_positions[metric]\n",
    "        plt.plot(\n",
    "            [i - 0.4, i + 0.4], \n",
    "            [total_count, total_count],  \n",
    "            color=\"black\" ,\n",
    "            linestyle=\"-\",\n",
    "            linewidth=2,\n",
    "            label=f\"Ground Truth\" if i == 0 else None\n",
    "        )\n",
    "\n",
    "\n",
    "plt.title(\"Confusion Matrix Values Across Models (with Ground Truth Lines)\", fontsize=18)\n",
    "plt.xlabel(\"Confusion Matrix Metric\", fontsize=18)\n",
    "plt.ylabel(\"Value\", fontsize=18)\n",
    "plt.xticks(rotation=45, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(title=\"Legend\", fontsize=18, title_fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/confusion_matrix_with_metric_specific_ground_truth_lines.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "sexist_tweets = df[df[\"SEXISM\"] == 1]\n",
    "print(\"Number of sexist tweets: \", len(sexist_tweets))\n",
    "non_sexist_tweets = df[df[\"SEXISM\"] == 0]\n",
    "print(\"Number of non-sexist tweets: \", len(non_sexist_tweets))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_all = vectorizer.fit_transform(df[\"text\"])  \n",
    "X_sexist = vectorizer.transform(sexist_tweets[\"text\"])  \n",
    "\n",
    "\n",
    "word_freq_all = X_all.sum(axis=0).A1  \n",
    "word_freq_sexist = X_sexist.sum(axis=0).A1\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "word_counts = pd.DataFrame({\n",
    "    \"word\": words,\n",
    "    \"sexist_count\": word_freq_sexist,\n",
    "    \"total_count\": word_freq_all\n",
    "})\n",
    "\n",
    "word_counts[\"P(sexist|word)\"] = word_counts[\"sexist_count\"] / word_counts[\"total_count\"]\n",
    "word_counts[\"P(word|sexist)\"] = word_counts[\"sexist_count\"] / word_freq_sexist.sum()\n",
    "word_counts[\"P(word)\"] = word_counts[\"total_count\"] / word_freq_all.sum()\n",
    "\n",
    "sorted_words = word_counts.sort_values(by=\"P(sexist|word)\", ascending=False)\n",
    "\n",
    "\n",
    "# print rows where sexist count is > 10\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "pd.set_option(\"display.width\", 1000)\n",
    "print(sorted_words[sorted_words[\"total_count\"] > 10].head(10)) \n",
    "\n",
    "# average p(sexist|word)\n",
    "print(\"Average P(sexist|word): \", sorted_words[\"P(sexist|word)\"].mean())\n",
    "print(len(sorted_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi Squared and P-value\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "\n",
    "sexist_tweets = df[df[\"SEXISM\"] == 1]\n",
    "non_sexist_tweets = df[df[\"SEXISM\"] == 0]\n",
    "total_sexist = len(sexist_tweets)\n",
    "total_non_sexist = len(non_sexist_tweets)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_all = vectorizer.fit_transform(df[\"text\"])  \n",
    "X_sexist = vectorizer.transform(sexist_tweets[\"text\"]) \n",
    "\n",
    "word_freq_all = X_all.sum(axis=0).A1  \n",
    "word_freq_sexist = X_sexist.sum(axis=0).A1  \n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "word_counts = pd.DataFrame({\n",
    "    \"word\": words,\n",
    "    \"sexist_count\": word_freq_sexist,\n",
    "    \"total_count\": word_freq_all\n",
    "})\n",
    "\n",
    "def calculate_chi_square(row):\n",
    "    word_present_sexist = row[\"sexist_count\"]\n",
    "    word_present_non_sexist = row[\"total_count\"] - row[\"sexist_count\"]\n",
    "    word_absent_sexist = total_sexist - word_present_sexist\n",
    "    word_absent_non_sexist = total_non_sexist - word_present_non_sexist\n",
    "    \n",
    "    contingency_table = [\n",
    "        [word_present_sexist, word_present_non_sexist],\n",
    "        [word_absent_sexist, word_absent_non_sexist]\n",
    "    ]\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    return pd.Series({\"chi2\": chi2, \"p_value\": p})\n",
    "\n",
    "\n",
    "word_counts[[\"chi2\", \"p_value\"]] = word_counts.apply(calculate_chi_square, axis=1)\n",
    "\n",
    "significant_words = word_counts.sort_values(by=\"p_value\", ascending=True)\n",
    "\n",
    "print(significant_words[significant_words[\"sexist_count\"] > 10].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
