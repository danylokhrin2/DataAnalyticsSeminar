{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conufsion Matrix and Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "results = []\n",
    "confusion_values = []\n",
    "\n",
    "model_columns = [\"gpt-4o-mini\", \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\", \"llama3.2-3b\", \"llama3.1-8b\", \"llama3-8b\"]\n",
    "\n",
    "for model in model_columns:\n",
    "    # Cast SEXISM to int\n",
    "    y_true = df['SEXISM'].astype(float).astype(int)\n",
    "    df[model] = pd.to_numeric(df[model], errors='coerce').fillna(0).astype(int)\n",
    "    y_pred = df[model]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"AUC\": auc\n",
    "    })\n",
    "    \n",
    "    confusion_values.append({\n",
    "        \"Model\": model,\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "confusion_df = pd.DataFrame(confusion_values)\n",
    "\n",
    "print(\"Confusion Matrix Values:\")\n",
    "print(confusion_df)\n",
    "print(\"\\nMetrics:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "results = []\n",
    "\n",
    "model_columns = [\"gpt-4o-mini\", \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\", \"llama3.2-3b\", \"llama3.1-8b\", \"llama3-8b\"]\n",
    "\n",
    "for model in model_columns:\n",
    "    # Cast SEXISM to int\n",
    "    y_true = df['SEXISM'].astype(float).astype(int)\n",
    "    df[model] = pd.to_numeric(df[model], errors='coerce').fillna(0).astype(int)\n",
    "    y_pred = df[model]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"AUC\": auc\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_melted = results_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "mean_scores = results_melted.groupby(\"Metric\")[\"Score\"].mean()\n",
    "plot_order = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC\"]\n",
    "mean_scores = mean_scores.reindex(plot_order)\n",
    "print(mean_scores)\n",
    "\n",
    "palette = {\n",
    "    \"gpt-4o-mini\": \"#1FDCB0\",\n",
    "    \"gpt-4-0125-preview\": \"#16A180\",\n",
    "    \"gpt-3.5-turbo-0125\": \"#0C5443\",\n",
    "    \"llama3.2-3b\": \"#0081FB\",\n",
    "    \"llama3.1-8b\": \"#005AAD\",\n",
    "    \"llama3-8b\": \"#003669\"\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(data=results_melted, x=\"Metric\", y=\"Score\", hue=\"Model\", palette=palette)\n",
    "\n",
    "for i, (metric, mean) in enumerate(mean_scores.items()):\n",
    "    plt.plot([i - 0.4, i + 0.4], [mean, mean], color='black', linestyle='-', linewidth=1.5, label=f\"Average\" if i == 0 else None)\n",
    "\n",
    "\n",
    "plt.title(\"Performance Metrics Across Models\", fontsize=18)\n",
    "plt.xlabel(\"Metric\", fontsize=18)\n",
    "plt.ylabel(\"Score\", fontsize=18)\n",
    "plt.xticks(rotation=45, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(title=\"Model\", fontsize=16, title_fontsize=18, bbox_to_anchor=(0.6, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/performance_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "confusion_values = []\n",
    "\n",
    "model_columns = [\"gpt-4o-mini\", \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\", \"llama3.2-3b\", \"llama3.1-8b\", \"llama3-8b\"]\n",
    "\n",
    "for model in model_columns:\n",
    "    # Cast SEXISM to int\n",
    "    y_true = df['SEXISM'].astype(float).astype(int)\n",
    "    df[model] = pd.to_numeric(df[model], errors='coerce').fillna(0).astype(int)\n",
    "    y_pred = df[model]\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    confusion_values.append({\n",
    "        \"Model\": model,\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "total_sexist = (df['SEXISM'] == 1).sum()\n",
    "total_non_sexist = (df['SEXISM'] == 0).sum()\n",
    "confusion_df = pd.DataFrame(confusion_values)\n",
    "confusion_melted = confusion_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "palette = {\n",
    "    \"gpt-4o-mini\": \"#1FDCB0\",\n",
    "    \"gpt-4-0125-preview\": \"#16A180\",\n",
    "    \"gpt-3.5-turbo-0125\": \"#0C5443\",\n",
    "    \"llama3.2-3b\": \"#0081FB\",\n",
    "    \"llama3.1-8b\": \"#005AAD\",\n",
    "    \"llama3-8b\": \"#003669\"\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(data=confusion_melted, x=\"Metric\", y=\"Value\", hue=\"Model\", palette=palette)\n",
    "\n",
    "metrics = confusion_melted[\"Metric\"].unique()\n",
    "metric_positions = {metric: i for i, metric in enumerate(metrics)}\n",
    "\n",
    "for metric, total_count in [(\"TP\", total_sexist), (\"TN\", total_non_sexist)]:\n",
    "    if metric in metric_positions:\n",
    "        i = metric_positions[metric]\n",
    "        plt.plot(\n",
    "            [i - 0.4, i + 0.4], \n",
    "            [total_count, total_count],  \n",
    "            color=\"black\" ,\n",
    "            linestyle=\"-\",\n",
    "            linewidth=2,\n",
    "            label=f\"Ground Truth\" if i == 0 else None\n",
    "        )\n",
    "\n",
    "\n",
    "plt.title(\"Confusion Matrix Values Across Models (with Ground Truth Lines)\", fontsize=18)\n",
    "plt.xlabel(\"Confusion Matrix Metric\", fontsize=18)\n",
    "plt.ylabel(\"Value\", fontsize=18)\n",
    "plt.xticks(rotation=45, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(title=\"Legend\", fontsize=18, title_fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/confusion_matrix_with_metric_specific_ground_truth_lines.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GROUND TRUTH\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "sexist_tweets = df[df[\"SEXISM\"] == 1]\n",
    "print(\"Number of sexist tweets: \", len(sexist_tweets))\n",
    "non_sexist_tweets = df[df[\"SEXISM\"] == 0]\n",
    "print(\"Number of non-sexist tweets: \", len(non_sexist_tweets))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_all = vectorizer.fit_transform(df[\"text\"])  \n",
    "X_sexist = vectorizer.transform(sexist_tweets[\"text\"])  \n",
    "\n",
    "\n",
    "word_freq_all = X_all.sum(axis=0).A1  \n",
    "word_freq_sexist = X_sexist.sum(axis=0).A1\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "word_counts = pd.DataFrame({\n",
    "    \"word\": words,\n",
    "    \"sexist_count\": word_freq_sexist,\n",
    "    \"total_count\": word_freq_all\n",
    "})\n",
    "\n",
    "word_counts[\"P(sexist|word)\"] = word_counts[\"sexist_count\"] / word_counts[\"total_count\"]\n",
    "word_counts[\"P(word|sexist)\"] = word_counts[\"sexist_count\"] / word_freq_sexist.sum()\n",
    "word_counts[\"P(word)\"] = word_counts[\"total_count\"] / word_freq_all.sum()\n",
    "\n",
    "sorted_words = word_counts.sort_values(by=\"P(sexist|word)\", ascending=False)\n",
    "\n",
    "\n",
    "# print rows where sexist count is > 10\n",
    "pd.set_option(\"display.max_columns\", None) \n",
    "pd.set_option(\"display.width\", 1000)\n",
    "print(sorted_words[sorted_words[\"total_count\"] > 10].head(10)) \n",
    "\n",
    "# average p(sexist|word)\n",
    "print(\"Average P(sexist|word): \", sorted_words[\"P(sexist|word)\"].mean())\n",
    "print(len(sorted_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi Squared and P-value\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "\n",
    "sexist_tweets = df[df[\"SEXISM\"] == 1]\n",
    "non_sexist_tweets = df[df[\"SEXISM\"] == 0]\n",
    "total_sexist = len(sexist_tweets)\n",
    "total_non_sexist = len(non_sexist_tweets)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_all = vectorizer.fit_transform(df[\"text\"])  \n",
    "X_sexist = vectorizer.transform(sexist_tweets[\"text\"]) \n",
    "\n",
    "word_freq_all = X_all.sum(axis=0).A1  \n",
    "word_freq_sexist = X_sexist.sum(axis=0).A1  \n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "word_counts = pd.DataFrame({\n",
    "    \"word\": words,\n",
    "    \"sexist_count\": word_freq_sexist,\n",
    "    \"total_count\": word_freq_all\n",
    "})\n",
    "\n",
    "def calculate_chi_square(row):\n",
    "    word_present_sexist = row[\"sexist_count\"]\n",
    "    word_present_non_sexist = row[\"total_count\"] - row[\"sexist_count\"]\n",
    "    word_absent_sexist = total_sexist - word_present_sexist\n",
    "    word_absent_non_sexist = total_non_sexist - word_present_non_sexist\n",
    "    \n",
    "    contingency_table = [\n",
    "        [word_present_sexist, word_present_non_sexist],\n",
    "        [word_absent_sexist, word_absent_non_sexist]\n",
    "    ]\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "    return pd.Series({\"chi2\": chi2, \"p_value\": p})\n",
    "\n",
    "\n",
    "word_counts[[\"chi2\", \"p_value\"]] = word_counts.apply(calculate_chi_square, axis=1)\n",
    "\n",
    "significant_words = word_counts.sort_values(by=\"p_value\", ascending=True)\n",
    "\n",
    "print(significant_words[significant_words[\"sexist_count\"] > 10].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-4o-mini\n",
      "           word  sexist_count  total_count  P(sexist|word)  P(word|sexist)   P(word)\n",
      "3411   feminazi            92           92        1.000000        0.005666  0.001615\n",
      "3408     female            18           19        0.947368        0.001109  0.000333\n",
      "1071    bitches            15           16        0.937500        0.000924  0.000281\n",
      "3416   feminist            13           14        0.928571        0.000801  0.000246\n",
      "10215     women            57           64        0.890625        0.003510  0.001123\n",
      "5763        men            43           52        0.826923        0.002648  0.000913\n",
      "477          an            23           28        0.821429        0.001416  0.000491\n",
      "3902       girl            35           43        0.813953        0.002155  0.000755\n",
      "664      asians            13           16        0.812500        0.000801  0.000281\n",
      "3060    english            12           15        0.800000        0.000739  0.000263\n",
      "Average P(sexist|word):  0.2695846903464188\n",
      "10422\n",
      "Model: gpt-4-0125-preview\n",
      "           word  sexist_count  total_count  P(sexist|word)  P(word|sexist)   P(word)\n",
      "3411   feminazi            90           92        0.978261        0.016408  0.001615\n",
      "3416   feminist            11           14        0.785714        0.002005  0.000246\n",
      "10215     women            49           64        0.765625        0.008933  0.001123\n",
      "8200        she            14           19        0.736842        0.002552  0.000333\n",
      "4266        her            16           24        0.666667        0.002917  0.000421\n",
      "5763        men            33           52        0.634615        0.006016  0.000913\n",
      "10214     woman            16           30        0.533333        0.002917  0.000527\n",
      "4971       just            14           29        0.482759        0.002552  0.000509\n",
      "651          as            13           33        0.393939        0.002370  0.000579\n",
      "2282       cunt           189          491        0.384929        0.034458  0.008618\n",
      "Average P(sexist|word):  0.09042055068080772\n",
      "10422\n",
      "Model: gpt-3.5-turbo-0125\n",
      "         word  sexist_count  total_count  P(sexist|word)  P(word|sexist)   P(word)\n",
      "6178    needs            23           23             1.0        0.000512  0.000404\n",
      "6149     nazi            16           16             1.0        0.000356  0.000281\n",
      "6068    music            20           20             1.0        0.000445  0.000351\n",
      "6259  niggers            11           11             1.0        0.000245  0.000193\n",
      "6254   niggas            21           21             1.0        0.000467  0.000369\n",
      "5691     mate            12           12             1.0        0.000267  0.000211\n",
      "5939    money            27           27             1.0        0.000601  0.000474\n",
      "6392   nobody            11           11             1.0        0.000245  0.000193\n",
      "6720      own            12           12             1.0        0.000267  0.000211\n",
      "6690      our            26           26             1.0        0.000578  0.000456\n",
      "Average P(sexist|word):  0.7485233041264544\n",
      "10422\n",
      "Model: llama3.2-3b\n",
      "         word  sexist_count  total_count  P(sexist|word)  P(word|sexist)   P(word)\n",
      "702        at            17           25        0.680000        0.001900  0.000439\n",
      "4266      her            16           24        0.666667        0.001789  0.000421\n",
      "9181    their            31           47        0.659574        0.003465  0.000825\n",
      "967     being            11           17        0.647059        0.001230  0.000298\n",
      "1437       by            11           17        0.647059        0.001230  0.000298\n",
      "10193    with            34           55        0.618182        0.003801  0.000965\n",
      "9992      was            17           28        0.607143        0.001900  0.000491\n",
      "6083       my            19           33        0.575758        0.002124  0.000579\n",
      "8034   screen            11           20        0.550000        0.001230  0.000351\n",
      "651        as            18           33        0.545455        0.002012  0.000579\n",
      "Average P(sexist|word):  0.1417556886231602\n",
      "10422\n",
      "Model: llama3.1-8b\n",
      "           word  sexist_count  total_count  P(sexist|word)  P(word|sexist)   P(word)\n",
      "5801    mexican            19           19        1.000000        0.000497  0.000333\n",
      "5624    manager            11           11        1.000000        0.000288  0.000193\n",
      "5259        lee            11           11        1.000000        0.000288  0.000193\n",
      "10139     whore            17           17        1.000000        0.000445  0.000298\n",
      "2289      cunts            15           15        1.000000        0.000392  0.000263\n",
      "1071    bitches            16           16        1.000000        0.000419  0.000281\n",
      "2875    dumbass            11           11        1.000000        0.000288  0.000193\n",
      "3411   feminazi            90           92        0.978261        0.002354  0.001615\n",
      "4266        her            23           24        0.958333        0.000602  0.000421\n",
      "3906      girls            21           22        0.954545        0.000549  0.000386\n",
      "Average P(sexist|word):  0.6489827666763198\n",
      "10422\n",
      "Model: llama3-8b\n",
      "           word  sexist_count  total_count  P(sexist|word)  P(word|sexist)   P(word)\n",
      "3416   feminist            14           14        1.000000        0.000516  0.000246\n",
      "10139     whore            16           17        0.941176        0.000590  0.000298\n",
      "3411   feminazi            86           92        0.934783        0.003171  0.001615\n",
      "4323        his            11           12        0.916667        0.000406  0.000211\n",
      "8200        she            17           19        0.894737        0.000627  0.000333\n",
      "5801    mexican            17           19        0.894737        0.000627  0.000333\n",
      "5526         lt            24           27        0.888889        0.000885  0.000474\n",
      "1071    bitches            14           16        0.875000        0.000516  0.000281\n",
      "4266        her            21           24        0.875000        0.000774  0.000421\n",
      "2282       cunt           429          491        0.873727        0.015819  0.008618\n",
      "Average P(sexist|word):  0.4568789312984946\n",
      "10422\n"
     ]
    }
   ],
   "source": [
    "# ALL MODELS\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "model_columns = [\"gpt-4o-mini\", \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\", \"llama3.2-3b\", \"llama3.1-8b\", \"llama3-8b\"]\n",
    "\n",
    "for model in model_columns:\n",
    "    print(f\"Model: {model}\")\n",
    "    sexist_tweets = df[df[model] == 1]\n",
    "    non_sexist_tweets = df[df[model] == 0]\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_all = vectorizer.fit_transform(df[\"text\"])  \n",
    "    X_sexist = vectorizer.transform(sexist_tweets[\"text\"])  \n",
    "\n",
    "\n",
    "    word_freq_all = X_all.sum(axis=0).A1  \n",
    "    word_freq_sexist = X_sexist.sum(axis=0).A1\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "    word_counts = pd.DataFrame({\n",
    "        \"word\": words,\n",
    "        \"sexist_count\": word_freq_sexist,\n",
    "        \"total_count\": word_freq_all\n",
    "    })\n",
    "\n",
    "    word_counts[\"P(sexist|word)\"] = word_counts[\"sexist_count\"] / word_counts[\"total_count\"]\n",
    "    word_counts[\"P(word|sexist)\"] = word_counts[\"sexist_count\"] / word_freq_sexist.sum()\n",
    "    word_counts[\"P(word)\"] = word_counts[\"total_count\"] / word_freq_all.sum()\n",
    "\n",
    "    sorted_words = word_counts.sort_values(by=\"P(sexist|word)\", ascending=False)\n",
    "\n",
    "\n",
    "    # print rows where sexist count is > 10\n",
    "    pd.set_option(\"display.max_columns\", None) \n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "    print(sorted_words[sorted_words[\"sexist_count\"] > 10].head(10)) \n",
    "\n",
    "    # average p(sexist|word)\n",
    "    print(\"Average P(sexist|word): \", sorted_words[\"P(sexist|word)\"].mean())\n",
    "    print(len(sorted_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
