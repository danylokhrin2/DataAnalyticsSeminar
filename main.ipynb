{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from LLM.gpt import send_requestGPT\n",
    "from LLM.gemini import send_requestGemini\n",
    "from LLM.llama import send_requestLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many unique texts there are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values_MASTER():\n",
    "    df = pd.read_csv(\"./data/preprocessed/MASTER.csv\")\n",
    "    unique_text_count = df['text'].nunique()\n",
    "    print(f\"Number of unique values in 'text' column: {unique_text_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count remaining Nan values in MASTER\n",
    "df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "# print for each column ['gpt-4o-mini','gpt-4-0125-preview','gpt-3.5-turbo-0125','llama3.2-3b','llama3.1-8b','llama3-8b','gemini-1.5-flash',]]\n",
    "print(f\"Remaining Texts 'gpt-4o-mini' column: \\t\\t {df['gpt-4o-mini'].isnull().sum()} out of {len(df)}\")\n",
    "print(f\"Remaining Texts 'gpt-4-0125-preview' column: \\t {df['gpt-4-0125-preview'].isnull().sum()} out of {len(df)}\")\n",
    "print(f\"Remaining Texts 'gpt-3.5-turbo-0125' column: \\t {df['gpt-3.5-turbo-0125'].isnull().sum()} out of {len(df)}\")\n",
    "print(f\"Remaining Texts 'llama3.2-3b' column: \\t\\t {df['llama3.2-3b'].isnull().sum()} out of {len(df)}\")\n",
    "print(f\"Remaining Texts 'llama3.1-8b' column: \\t\\t {df['llama3.1-8b'].isnull().sum()} out of {len(df)}\")\n",
    "print(f\"Remaining Texts 'llama3-8b' column: \\t\\t {df['llama3-8b'].isnull().sum()} out of {len(df)}\")\n",
    "print(f\"Remaining Texts 'gemini-1.5-flash' column: \\t {df['gemini-1.5-flash'].isnull().sum()} out of {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_row(model):\n",
    "    \n",
    "    df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "    system_prompt = \"You are an assistant trained to identify if text contains sexism. Answer ONLY with '1' for Yes or '0' for No. ONLY CHECK FOR SEXISM AND NOT OTHER FORMS OF HATE SPEECH.\"\n",
    "    \n",
    "    nan_rows = df[df[model].isna()]\n",
    "\n",
    "    if not nan_rows.empty:\n",
    "        random_index = random.choice(nan_rows.index)\n",
    "        text_value = df.at[random_index, 'text']\n",
    "        sexism = df.at[random_index, 'SEXISM']\n",
    "        if \"gemini\" in model:\n",
    "            value = send_requestGemini(model, system_prompt, text_value)\n",
    "        elif \"llama\" in model:\n",
    "            value = send_requestLlama(model, system_prompt, text_value)            \n",
    "        else:\n",
    "            value = send_requestGPT(model, system_prompt, text_value)\n",
    "        df.at[random_index, model] = value\n",
    "        print(f\"Populated NaN at index {random_index + 1} with value {value} vs. {sexism} for text: {text_value}\") \n",
    "    else:\n",
    "        print(f\"No NaN values found in {model} column.\")\n",
    "\n",
    "    df.to_csv(\"./data/result/MASTER.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model options: \"gpt-4o-mini\", \"gpt-4-0125-preview\", \"gpt-3.5-turbo-0125\"\n",
    "for i in range(1000):\n",
    "    print(str(i) + \": \")\n",
    "    populate_row(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA \n",
    "for i in range(1000):\n",
    "    populate_row(\"llama3.2-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEMINI\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        populate_row(\"gemini-1.5-flash\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    time.sleep(4) # to avoid rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARALLELISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def populate_row_for_models(row):\n",
    "    system_prompt = \"You are an assistant trained to identify if text contains sexism. Answer ONLY with '1' for Yes or '0' for No. ONLY CHECK FOR SEXISM AND NOT OTHER FORMS OF HATE SPEECH.\"\n",
    "    \n",
    "    # Get text and other details\n",
    "    text_value = row['text']\n",
    "    sexism = row['SEXISM']\n",
    "    \n",
    "    # Process each model's NaN separately\n",
    "    for model in ['gpt-4o-mini','gpt-4-0125-preview','gpt-3.5-turbo-0125','llama3.2-3b','llama3.1-8b','llama3-8b']:\n",
    "        if pd.isna(row[model]):\n",
    "            if \"gemini\" in model:\n",
    "                try:\n",
    "                  row[model] = send_requestGemini(model, system_prompt, text_value)\n",
    "                except:\n",
    "                  row[model] = -1\n",
    "            elif \"llama\" in model:\n",
    "                row[model] = send_requestLlama(model, system_prompt, text_value)\n",
    "            else:\n",
    "                row[model] = send_requestGPT(model, system_prompt, text_value)\n",
    "            \n",
    "    print(f\"Row {row['ID']} processed\")\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Main function to populate rows with NaN in parallel\n",
    "def populate_rows_in_parallel(max_rows):\n",
    "    df = pd.read_csv(\"./data/result/MASTER.csv\")\n",
    "\n",
    "    # Filter rows with NaN in any model column\n",
    "    rows_with_nan = df[df[['gpt-4o-mini','gpt-4-0125-preview','gpt-3.5-turbo-0125','llama3.2-3b','llama3.1-8b','llama3-8b']].isna().any(axis=1)]\n",
    "\n",
    "    rows_with_nan = rows_with_nan.head(max_rows)\n",
    "\n",
    "    # Process each row in parallel and update the DataFrame\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        updated_rows = list(executor.map(populate_row_for_models, [row for _, row in rows_with_nan.iterrows()]))\n",
    "\n",
    "    # Update the original DataFrame with the modified rows\n",
    "    for updated_row in updated_rows:\n",
    "        df.loc[df['ID'] == updated_row['ID'], updated_row.index] = updated_row.values\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV\n",
    "    df.to_csv(\"./data/result/MASTER.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_rows_in_parallel(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
